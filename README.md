# Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors

## Abstract

The propensity of abstractive summarization systems to make factual errors has been the subject of significant study, including work on models to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, error detectors, and annotated benchmarks make factuality evaluation a moving target; it is hard to get a clear picture of how techniques compare. In this work, we collect labeled factuality errors from across nine datasets of annotated summary outputs and stratify them in a new way, focusing on what kind of base summarization model was used. To support finer-grained analysis, we unify the labeled error types into a single taxonomy and project each of the datasets' errors into this shared labeled space. We then contrast five state-of-the-art error detection methods on this benchmark. Our findings show that benchmarks built on modern summary outputs (those from pre-trained models) show significantly different results than benchmarks using pre-Transformer models. Furthermore, no one factuality technique is superior in all settings or for all error types, suggesting that system developers should take care to choose the right system for their task at hand.

Please see our full paper [here](https://arxiv.org/pdf/2205.12854v1.pdf).

## AggreFact Benchmark

The dataset can be found in the `data` folder. `aggre_fact_sota.csv` is a subset of `aggre_fact_final.csv`, which only contains summaries generated by SOTA models. The followings are descriptions of column names.

|Col. name | Description |
|--|--|
| origin | Summarization dataset. Either *cnndm* or *xsum*. |
|id| Document id.|
|doc| Input article. |
|summary| model generated summary.|
|model_name| Name of the model used to generate the summary|
|label|Factual consistency of the generated summary. 1 is factually consistent, *0* otherwise.|
|cut| Either *val* or *test*.|
|*system*_score| The output score from a *factuality system*.|
|*system*_label| The binary factual consistency label based on the score of the factuality system. Only examples in the *test* set have labels. Labels are determined under the *threshold-per-dataset* setting.
